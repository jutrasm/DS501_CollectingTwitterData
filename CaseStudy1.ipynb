{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study 1 : Collecting Data from Twitter\n",
    "\n",
    "Due Date: September 22, **before the beginning of class at 6:00pm**\n",
    "\n",
    "* ------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/en/thumb/9/9f/Twitter_bird_logo_2012.svg/220px-Twitter_bird_logo_2012.svg.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "**TEAM Members:** Please EDIT this cell and add the names of all the team members in your team\n",
    "\n",
    "    Melanie Jutras\n",
    "    \n",
    "    Debraj Roy\n",
    "    \n",
    "    Praneeth Nooli\n",
    "    \n",
    "    Rahul Ghadge\n",
    "    \n",
    "    Sushma Tayanna\n",
    "\n",
    "**Required Readings:** \n",
    "* Chapter 1 and Chapter 9 of the book [Mining the Social Web](http://www.learndatasci.com/wp-content/uploads/2015/08/Mining-the-Social-Web-2nd-Edition.pdf) \n",
    "* The codes for [Chapter 1](http://bit.ly/1qCtMrr) and [Chapter 9](http://bit.ly/1u7eP33)\n",
    "\n",
    "\n",
    "** NOTE **\n",
    "* Please don't forget to save the notebook frequently when working in IPython Notebook, otherwise the changes you made can be lost.\n",
    "\n",
    "*----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Sampling Twitter Data with Streaming API about a certain topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Select a topic that you are interested in, for example, \"WPI\" or \"Lady Gaga\"\n",
    "* Use Twitter Streaming API to sample a collection of tweets about this topic in real time. (It would be recommended that the number of tweets should be larger than 200, but smaller than 1 million.\n",
    "* Store the tweets you downloaded into a local file (txt file or json file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<twitter.api.Twitter object at 0x00000208055E30F0>\n"
     ]
    }
   ],
   "source": [
    "import twitter\n",
    "import json\n",
    "import pymongo\n",
    "\n",
    "#--------------------------------------------------------------------------------------\n",
    "# The purpose of oauth_login() function is\n",
    "#    1. Login to the Twitter API\n",
    "#    2. Return twitter handle necessary for its use in later functions\n",
    "\n",
    "def oauth_login():\n",
    "    # Had to go to http://twitter.com/apps/new to create an app and get values\n",
    "    # for these credentials\n",
    "    # See https://dev.twitter.com/docs/auth/oauth for more information \n",
    "    # on Twitter's OAuth implementation.\n",
    "    \n",
    "    CONSUMER_KEY = 'zmbyPowgnW1QNs9LXjGtNqMcm'\n",
    "    CONSUMER_SECRET ='wnbgjQQJxidVvnw3CORSw5MVjbOE8KstdXlkPLrH01R9JLAlPn'\n",
    "    OAUTH_TOKEN = '1529017262-SoyYbWBJWW8Mn0MLuyQLpmViknAiGQg1zVJv3N7'\n",
    "    OAUTH_TOKEN_SECRET = '4nqZePogkTfNVBJqqb1VN0dpprz8ykeKBDrdxXsxDFYnC'\n",
    "    \n",
    "    auth = twitter.oauth.OAuth(OAUTH_TOKEN, OAUTH_TOKEN_SECRET,\n",
    "                               CONSUMER_KEY, CONSUMER_SECRET)\n",
    "    \n",
    "    twitter_api = twitter.Twitter(auth=auth)\n",
    "    return twitter_api\n",
    "#--------------------------------------------------------------------------------------\n",
    "# The purpose of save_to_mongo() function is\n",
    "#    1. Connect to the MongoDB server running on localhost\n",
    "#    2. Get a reference to database specified in input paramter\n",
    "#    3. Reference collection specified in input parameter\n",
    "#    4. Save data to database and return IDs\n",
    "\n",
    "def save_to_mongo(data, mongo_db, mongo_db_coll, **mongo_conn_kw):\n",
    "    \n",
    "    # Connects to the MongoDB server running on \n",
    "    # localhost:27017 by default\n",
    "    \n",
    "    client = pymongo.MongoClient(**mongo_conn_kw)\n",
    "    \n",
    "    # Get a reference to a particular database\n",
    "    \n",
    "    db = client[mongo_db]\n",
    "    \n",
    "    # Reference a particular collection in the database\n",
    "    \n",
    "    coll = db[mongo_db_coll]\n",
    "    \n",
    "    # Perform a bulk insert and  return the IDs\n",
    "    return coll.insert_many(data)\n",
    "#--------------------------------------------------------------------------------------\n",
    "# The purpose of load_from_mongo() function is\n",
    "#    1. Given the db and coll passed in, find data based on criteria if given\n",
    "#    4. Load data from database and return cursor or item to user\n",
    "\n",
    "def load_from_mongo(mongo_db, mongo_db_coll, return_cursor=False,\n",
    "                    criteria=None, projection=None, **mongo_conn_kw):\n",
    "    \n",
    "    # Optionally, use criteria and projection to limit the data that is \n",
    "    # returned as documented in \n",
    "    # http://docs.mongodb.org/manual/reference/method/db.collection.find/\n",
    "    \n",
    "    # Consider leveraging MongoDB's aggregations framework for more \n",
    "    # sophisticated queries.\n",
    "    \n",
    "    client = pymongo.MongoClient(**mongo_conn_kw)\n",
    "    db = client[mongo_db]\n",
    "    coll = db[mongo_db_coll]\n",
    "    \n",
    "    if criteria is None:\n",
    "        criteria = {}\n",
    "    \n",
    "    if projection is None:\n",
    "        cursor = coll.find(criteria)\n",
    "    else:\n",
    "        cursor = coll.find(criteria, projection)\n",
    "\n",
    "    # Returning a cursor is recommended for large amounts of data\n",
    "    \n",
    "    if return_cursor:\n",
    "        return cursor\n",
    "    else:\n",
    "        return [ item for item in cursor ]\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------\n",
    "# The purpose of the get_trends() function is:\n",
    "#    1. Gather current world trends from twitter\n",
    "#    2. Gather current US trends from twitter\n",
    "#    3. Print the trends for viewing (TODO: Do we want to save trending data?)\n",
    "\n",
    "def get_trends(twitter_api):\n",
    "    \n",
    "    # The Yahoo! Where On Earth ID for the entire world is 1.\n",
    "    # See https://dev.twitter.com/docs/api/1.1/get/trends/place and\n",
    "    # http://developer.yahoo.com/geo/geoplanet/\n",
    "\n",
    "    WORLD_WOE_ID = 1\n",
    "    US_WOE_ID = 23424977\n",
    "\n",
    "    # Prefix ID with the underscore for query string parameterization.\n",
    "    # Without the underscore, the twitter package appends the ID value\n",
    "    # to the URL itself as a special case keyword argument.\n",
    "\n",
    "    world_trends = twitter_api.trends.place(_id=WORLD_WOE_ID)\n",
    "    us_trends = twitter_api.trends.place(_id=US_WOE_ID)\n",
    "\n",
    "\n",
    "    print ('*** Here are the WORLD TRENDS :')\n",
    "    print (json.dumps(world_trends, indent=1))\n",
    "    print ('*** Here are the US TRENDS :')\n",
    "    print (json.dumps(us_trends, indent=1))\n",
    "\n",
    "#--------------------------------------------------------------------------------------\n",
    "# OLD CODE - NO LONGER USED\n",
    "# The purpose of the getPokemonGoTweets() function is:\n",
    "#   1. Search twitter for tweets containing the search term '#PokemonGo'\n",
    "#   2. Save any tweets retrieved in a file named 'pokemonresults.txt'\n",
    "#\n",
    "# This was the original function written to get PokemonGo tweets.  It is no longer\n",
    "# being called by anyone, but was saved to show our first attempt using the polling API\n",
    "# and writing results to a file rather than a database.\n",
    "\n",
    "def getPokemonGoTweets(twitter_api):\n",
    "    \n",
    "       \n",
    "    # Import unquote to prevent url encoding errors in next_results\n",
    "    from urllib.parse import unquote\n",
    "\n",
    "    # Set q to the topic of interest for CaseStudy1-Problem1, PokemonGo \n",
    "\n",
    "    q = '#PokemonGo' \n",
    "\n",
    "    count = 100\n",
    "\n",
    "    # See https://dev.twitter.com/docs/api/1.1/get/search/tweets\n",
    "\n",
    "    search_results = twitter_api.search.tweets(q=q, count=count)\n",
    "\n",
    "    statuses = search_results['statuses']\n",
    "\n",
    "\n",
    "    # Iterate through 5 more batches of results by following the cursor\n",
    "\n",
    "    for _ in range(5):\n",
    "        print (\"Length of statuses\", len(statuses))\n",
    "        try:\n",
    "            next_results = search_results['search_metadata']['next_results']\n",
    "        except KeyError as e: # No more results when next_results doesn't exist\n",
    "            break\n",
    "\n",
    "        # Create a dictionary from next_results, which has the following form:\n",
    "        # ?max_id=313519052523986943&q=NCAA&include_entities=1\n",
    "        kwargs = dict([ kv.split('=') for kv in unquote(next_results[1:]).split(\"&\") ])\n",
    "\n",
    "        search_results = twitter_api.search.tweets(**kwargs)\n",
    "        statuses += search_results['statuses']\n",
    "        \n",
    "    # Dump data to json file in json format    \n",
    "    with open('pokemonresults.txt', 'w') as outfile:\n",
    "        json.dump(statuses, outfile)  \n",
    "    # Note: by opening file using 'with', no need to close() the file. It happens implicitly.\n",
    "    \n",
    "\n",
    "#--------------------------------------------------------------------------------------\n",
    "# The purpose of the twitter_search() function is:\n",
    "#   1. Search twitter for tweets containing the search term q, defaulting results=200\n",
    "#      because oAuth users can \"only\" make 180 search queries per 15 minute interval\n",
    "#\n",
    "#   2. return statuses\n",
    "\n",
    "def twitter_search(twitter_api, q, max_results=200, **kw):\n",
    "\n",
    "    # See https://dev.twitter.com/docs/api/1.1/get/search/tweets and \n",
    "    # https://dev.twitter.com/docs/using-search for details on advanced \n",
    "    # search criteria that may be useful for keyword arguments\n",
    "    \n",
    "    # See https://dev.twitter.com/docs/api/1.1/get/search/tweets    \n",
    "    search_results = twitter_api.search.tweets(q=q, count=100, **kw)\n",
    "    \n",
    "    statuses = search_results['statuses']\n",
    "    \n",
    "    # Iterate through batches of results by following the cursor until we\n",
    "    # reach the desired number of results, keeping in mind that OAuth users\n",
    "    # can \"only\" make 180 search queries per 15-minute interval. See\n",
    "    # https://dev.twitter.com/docs/rate-limiting/1.1/limits\n",
    "    # for details. A reasonable number of results is ~1000, although\n",
    "    # that number of results may not exist for all queries.\n",
    "    \n",
    "    # Enforce a reasonable limit\n",
    "    max_results = min(1000, max_results)\n",
    "    \n",
    "    for _ in range(10): # 10*100 = 1000\n",
    "        try:\n",
    "            next_results = search_results['search_metadata']['next_results']\n",
    "        except KeyError as e: # No more results when next_results doesn't exist\n",
    "            break\n",
    "            \n",
    "        # Create a dictionary from next_results, which has the following form:\n",
    "        # ?max_id=313519052523986943&q=NCAA&include_entities=1\n",
    "        kwargs = dict([ kv.split('=') \n",
    "                        for kv in next_results[1:].split(\"&\") ])\n",
    "        \n",
    "        search_results = twitter_api.search.tweets(**kwargs)\n",
    "        statuses += search_results['statuses']\n",
    "        \n",
    "        if len(statuses) > max_results: \n",
    "            break\n",
    "            \n",
    "    return statuses\n",
    "\n",
    "        \n",
    "\n",
    "#--------------------------------------------------------------------------------------\n",
    "# The purpose of the mainDataGathering() function is:\n",
    "#     1. Login to twitter and get api handle\n",
    "#     1. Gather desired twitter data and save for future use\n",
    "\n",
    "def mainDataGathering():\n",
    "    \n",
    "    twitter_api = oauth_login()\n",
    "    print(twitter_api)\n",
    "    \n",
    "    # Search twitter for \"#PokemonGo\" tweets\n",
    "    q = \"#PokemonGo\"\n",
    "\n",
    "    # Use twitter streaming API\n",
    "    twitter_stream = twitter.TwitterStream(auth=twitter_api.auth)\n",
    "    \n",
    "    # See https://dev.twitter.com/docs/streaming-apis\n",
    "    #stream = twitter_stream.statuses.filter(track=q)\n",
    "    \n",
    "    \n",
    "    #get_trends(twitter_api)\n",
    "\n",
    "    #getPokemonGoTweets(twitter_api)\n",
    "    \n",
    "    # MongoDB---------------------------------------------------\n",
    "    # Connects to the MongoDB server running on \n",
    "    # localhost:27017 by default\n",
    "    \n",
    "    #client = pymongo.MongoClient(**mongo_conn_kw)\n",
    "    client = pymongo.MongoClient('localhost', 27017)\n",
    "    \n",
    "    # Get a reference to a particular database\n",
    "    mongo_db = 'stream_search_results'\n",
    "    db = client[mongo_db]\n",
    "    \n",
    "    # Reference a particular collection in the database\n",
    "    mongo_db_coll = 'StreamingTweet'\n",
    "    coll = db[mongo_db_coll]\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #results = twitter_search(twitter_api, q, max_results=10)\n",
    "    stream = twitter_stream.statuses.filter(track=q)\n",
    "    for tweet in stream:\n",
    "        #print(tweet['text'])  \n",
    "        # Save results to MongoDB\n",
    "        coll.insert_one(tweet)\n",
    "        #save_to_mongo(stream, 'stream_search_results', 'StreamingTweet')\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # This is an example of how to get some data from the database\n",
    "    #\n",
    "    #print('**** Load the DATABASE ***')\n",
    "    #cursor = load_from_mongo('stream_search_results', q, return_cursor=True)\n",
    "    #print('**** HERE is some data from the DATABASE ***')\n",
    "    #for trend in cursor:\n",
    "        # Print the text\n",
    "        # print(trend['text'])\n",
    "        # Print user and location\n",
    "        #print(trend['user']['location'])\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "#--------------------------------------------------------------------------------------\n",
    "# Call the mainDataGathering() function to perform actions needed for \n",
    "# DS501 CaseStudy 1 Problem 1 : Sampling twitter data with streaming api about a topic\n",
    "\n",
    "mainDataGathering()\n",
    "\n",
    "#--------------------------------------------------------------------------------------\n",
    "# Test that the mainDataGathering() worked by displaying a sample result from the file\n",
    "# This is not necessary and can be commented out if display is not desired here...\n",
    "\n",
    "#print (\"*** Sample PokemonGo output : \")\n",
    "# Get one sample search result from file\n",
    "#with open('pokemonresults.txt','r') as json_data:\n",
    "#    json_statuses = json.load(json_data)\n",
    "#    print(json.dumps(json_statuses[0], indent=4))\n",
    "# Note: by opening file using 'with', no need to close() the file. It happens implicitly.\n",
    "\n",
    "# q = \"#PokemonGo\"\n",
    "#load_from_mongo('stream_search_results', q)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Report some statistics about the tweets you collected \n",
    "\n",
    "* The topic of interest: < #PokemonGo >\n",
    "\n",
    "\n",
    "* The total number of tweets collected:  < 15246 >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Analyzing Tweets and Tweet Entities with Frequency Analysis\n",
    "\n",
    "**1. Word Count:** \n",
    "* Use the tweets you collected in Problem 1, and compute the frequencies of the words being used in these tweets. \n",
    "* Plot a table of the top 30 words with their counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Load the DATABASE ***\n",
      "**** HERE is some data from the DATABASE ***\n",
      "[\n",
      " \"Dalia  #Pokemon #App #PokemonApp #PokemonGo #Bulbasur #Planta #PokemonPretty #Pretty #TipoPlanta\\u2026 https://t.co/FUOYy06VQ6\",\n",
      " \"ALERTE!\\nGoupix vient d'appara\\u00eetre au Parc mon Repos!\\nIl va dispara\\u00eetre \\u00e0 14:26!\\nhttps://t.co/13Je50XBGn #Goupix #ParcmonRepos #PokemonGo\",\n",
      " \"Leveinard \\u00e0 4 Avenue Joannes Hubert, 69160 jusqu'\\u00e0 14:17:32. #PokemonGo https://t.co/WgEF7ZX17R\",\n",
      " \"#PokemonGo #Share: New - Shipping Today - #Pokemon Bracelet Go Plus Bluetooth Device https://t.co/zcpqC5Nk3N https://t.co/cFb6f4jstx\",\n",
      " \"A wild Electabuzz appeared! It will be in Uptown Waterloo until 8:26 AM. https://t.co/Yw7y1VwFKn #Electabuzz #UptownWaterloo #PokemonGo #KW\"\n",
      "]\n",
      "[\n",
      " \"ElecEveryday\",\n",
      " \"YouTube\",\n",
      " \"SafariZoneGo\",\n",
      " \"N3brios\",\n",
      " \"EpicBoxChile\"\n",
      "]\n",
      "[\n",
      " \"Pokemon\",\n",
      " \"App\",\n",
      " \"PokemonApp\",\n",
      " \"PokemonGo\",\n",
      " \"Bulbasur\"\n",
      "]\n",
      "[\n",
      " \"Dalia\",\n",
      " \"#Pokemon\",\n",
      " \"#App\",\n",
      " \"#PokemonApp\",\n",
      " \"#PokemonGo\"\n",
      "]\n",
      "\n",
      "*** And here are the TOP 30 : \n",
      "+------------+-------+\n",
      "| Words      | Count |\n",
      "+------------+-------+\n",
      "| #PokemonGo |  5357 |\n",
      "| #PokemonGO |  5109 |\n",
      "| until      |  3288 |\n",
      "| A          |  3208 |\n",
      "| wild       |  2982 |\n",
      "| appeared!  |  2946 |\n",
      "| be         |  2781 |\n",
      "| will       |  2758 |\n",
      "| It         |  2593 |\n",
      "| RT         |  2365 |\n",
      "| you        |  2278 |\n",
      "| #pokemongo |  1600 |\n",
      "| Pokecoins? |  1584 |\n",
      "| AM.        |  1308 |\n",
      "| need       |  1261 |\n",
      "| Do         |  1245 |\n",
      "| the        |  1241 |\n",
      "| Try        |  1214 |\n",
      "| to         |  1144 |\n",
      "| in         |  1005 |\n",
      "| a          |   866 |\n",
      "| at         |   830 |\n",
      "| near       |   812 |\n",
      "| I          |   812 |\n",
      "| Rattata.   |   775 |\n",
      "| PM.        |   765 |\n",
      "| which      |   754 |\n",
      "| IN:        |   749 |\n",
      "| SIGN       |   749 |\n",
      "| Discover   |   749 |\n",
      "+------------+-------+\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------\n",
    "# Your code starts here\n",
    "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
    "#   Please feel free to add more cells below this cell if necessary\n",
    "\n",
    "import json\n",
    "import pymongo\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "mongo_db = 'stream_search_results'\n",
    "mongo_db_coll = 'StreamingTweet'\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# This is an example of how to get some data from the database\n",
    "#\n",
    "print('**** Load the DATABASE ***')\n",
    "#cursor = load_from_mongo('stream_search_results', 'twitter', return_cursor=True)\n",
    "# Connects to the MongoDB server running on \n",
    "# localhost:27017 by default\n",
    "    \n",
    "client = pymongo.MongoClient()\n",
    "    \n",
    "# Get a reference to a particular database\n",
    "    \n",
    "db = client[mongo_db]\n",
    "    \n",
    "# Reference a particular collection in the database\n",
    "coll = db[mongo_db_coll]\n",
    "\n",
    "cursor = coll.find({})\n",
    "\n",
    "\n",
    "print('**** HERE is some data from the DATABASE ***')\n",
    "\n",
    "#for trend in cursor:\n",
    "    #print(trend['text'])\n",
    "    #print(trend['user']['location'])\n",
    "    \n",
    "\n",
    "     \n",
    "# This code is based on Example #10-13 in Chapter 9 of Mining the Social Web\n",
    "    \n",
    "\n",
    "\n",
    "curr_tweet_texts = [ curr_tweet['text'] \n",
    "                     for curr_tweet in cursor ]\n",
    "\n",
    "cursor = coll.find({})\n",
    "\n",
    "screen_names = [ user_mention['screen_name'] \n",
    "                 for curr_tweet in cursor\n",
    "                     for user_mention in curr_tweet['entities']['user_mentions'] ]\n",
    "\n",
    "cursor = coll.find({})\n",
    "\n",
    "hashtags = [ hashtag['text'] \n",
    "             for curr_tweet in cursor\n",
    "                 for hashtag in curr_tweet['entities']['hashtags'] ]\n",
    "\n",
    "# Compute a collection of all words from all tweets\n",
    "words = [ w \n",
    "          for t in curr_tweet_texts \n",
    "              for w in t.split() ]\n",
    "\n",
    "# Explore the first 5 items for each...\n",
    "\n",
    "print (json.dumps(curr_tweet_texts[0:5], indent=1))\n",
    "print (json.dumps(screen_names[0:5], indent=1)) \n",
    "print (json.dumps(hashtags[0:5], indent=1))\n",
    "print (json.dumps(words[0:5], indent=1))\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "print()\n",
    "print('*** And here are the TOP 30 : ')\n",
    "\n",
    "for item in [words]:\n",
    "    pt = PrettyTable(field_names=['Words', 'Count'])\n",
    "    c = Counter(item)\n",
    "    #print (c.most_common()[:30]) # top 30\n",
    "    [ pt.add_row(kv) for kv in c.most_common()[:30] ]\n",
    "    pt.align['Words'], pt.align['Count'] = 'l', 'r'\n",
    "    print(pt)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Find the most popular tweets in your collection of tweets**\n",
    "\n",
    "Please plot a table of the top 10 tweets that are the most popular among your collection, i.e., the tweets with the largest number of retweet counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------\n",
    "# Your code starts here\n",
    "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
    "#   Please feel free to add more cells below this cell if necessary\n",
    "\n",
    "\n",
    "# Example 10 Ch9\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Find the most popular Tweet Entities in your collection of tweets**\n",
    "\n",
    "Please plot a table of the top 10 hashtags, top 10 user mentions that are the most popular in your collection of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------\n",
    "# Your code starts here\n",
    "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
    "#   Please feel free to add more cells below this cell if necessary\n",
    "\n",
    "\n",
    "# Example 14 Ch9\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ------------------------\n",
    "\n",
    "# Problem 3: Getting \"All\" friends and \"All\" followers of a popular user in twitter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* choose a popular twitter user who has many followers, such as \"ladygaga\".\n",
    "* Get the list of all friends and all followers of the twitter user.\n",
    "* Plot 20 out of the followers, plot their ID numbers and screen names in a table.\n",
    "* Plot 20 out of the friends (if the user has more than 20 friends), plot their ID numbers and screen names in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetched 327 total friends ids for @JohnHanke\n",
      "Fetched 5000 total followers ids for @JohnHanke\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** HERE are ids of mutual friends followers of  @JohnHanke  *** \n",
      "[1724473160, 769080580302569473, 130379779, 18327902]\n",
      "*** HERE are  22  ids of friends of  @JohnHanke  *** \n",
      "[2244340904, 816653, 1636590253, 769080580302569473, 1724473160, 37666984, 105912240, 6331462, 1394399438, 16303106, 20106796, 2227174303, 81285400, 2839430431, 21094279, 714975751, 69489820, 14188906, 96879107, 19511752, 631577690, 714249583853867009]\n",
      "*** HERE are  22  ids of followers of  @JohnHanke  *** \n",
      "[403424548, 113594235, 577964386, 217434145, 4914835684, 759887823038214144, 4765106839, 720515406532714496, 778336178621800448, 2164467097, 778358394063388677, 232204464, 58844519, 1673113746, 2970697108, 1211273371, 778318908910477312, 2204146394, 1391281, 2726900475, 2696492256, 3279871050]\n",
      "\n",
      "*** Let's see them in Pretty Tables ! Doesn't this look Better ? *** \n",
      "\n",
      "+--------------------+--------------------+\n",
      "|          Friend ID | Friend Screen Name |\n",
      "+--------------------+--------------------+\n",
      "|             816653 |         TechCrunch |\n",
      "|         1636590253 |           tim_cook |\n",
      "| 769080580302569473 |        AgentKodama |\n",
      "|         1724473160 |         Skiplagged |\n",
      "|           37666984 |         nikkihaley |\n",
      "|          105912240 |          jimsteyer |\n",
      "|            6331462 |               jess |\n",
      "|         1394399438 |         JohnLegere |\n",
      "|           16303106 |      StephenAtHome |\n",
      "|           20106796 |        amberkanwar |\n",
      "|         2227174303 |      leanne_graves |\n",
      "|           81285400 |        _AlexHirsch |\n",
      "|         2839430431 |       PokemonGoApp |\n",
      "|           21094279 |          chillmage |\n",
      "|          714975751 |        TweetMcCool |\n",
      "|           69489820 |         erickasoTV |\n",
      "|           14188906 |            BryFitz |\n",
      "|           96879107 |            Pokemon |\n",
      "|           19511752 |          GabrielJR |\n",
      "|          631577690 |        NianticLabs |\n",
      "+--------------------+--------------------+\n",
      "+--------------------+----------------------+\n",
      "|        Follower ID | Follower Screen Name |\n",
      "+--------------------+----------------------+\n",
      "|          113594235 |         eykinakamuta |\n",
      "|          577964386 |      yukarinmurasaki |\n",
      "|          217434145 |              ehdante |\n",
      "|         4914835684 |             AkaVevie |\n",
      "| 759887823038214144 |       Miguel69581435 |\n",
      "|         4765106839 |        valdenirluiz1 |\n",
      "| 720515406532714496 |      pokegotakatsuki |\n",
      "| 778336178621800448 |         argunbey1071 |\n",
      "|         2164467097 |             mikewsyd |\n",
      "| 778358394063388677 |      lucky_lucario16 |\n",
      "|          232204464 |             gusgompy |\n",
      "|           58844519 |             prosimma |\n",
      "|         1673113746 |          garzarnancy |\n",
      "|         2970697108 |          Meeper12346 |\n",
      "|         1211273371 |         buddhablue21 |\n",
      "| 778318908910477312 |       Metro_Instinct |\n",
      "|         2204146394 |          Domanated89 |\n",
      "|            1391281 |        the_fresh_one |\n",
      "|         2726900475 |           lhawkins90 |\n",
      "|         2696492256 |            ismar6300 |\n",
      "+--------------------+----------------------+\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------------------------------------\n",
    "# The purpose of this code is to get \"all\" friends and \"all\" followers of a popular\n",
    "# twitter user.  This code is based on Example 19 in Chapter 9 of the book, Mining the\n",
    "# Social Web.\n",
    "\n",
    "from functools import partial\n",
    "from sys import maxsize\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "\n",
    "import sys\n",
    "import time\n",
    "from urllib.error import URLError\n",
    "from http.client import BadStatusLine\n",
    "#import json\n",
    "#import twitter\n",
    "\n",
    "def make_twitter_request(twitter_api_func, max_errors=10, *args, **kw): \n",
    "    \n",
    "    # A nested helper function that handles common HTTPErrors. Return an updated\n",
    "    # value for wait_period if the problem is a 500 level error. Block until the\n",
    "    # rate limit is reset if it's a rate limiting issue (429 error). Returns None\n",
    "    # for 401 and 404 errors, which requires special handling by the caller.\n",
    "    def handle_twitter_http_error(e, wait_period=2, sleep_when_rate_limited=True):\n",
    "    \n",
    "        if wait_period > 3600: # Seconds\n",
    "            print ('Too many retries. Quitting.', file = sys.stderr)\n",
    "            raise e\n",
    "    \n",
    "        # See https://dev.twitter.com/docs/error-codes-responses for common codes\n",
    "    \n",
    "        if e.e.code == 401:\n",
    "            print ('Encountered 401 Error (Not Authorized)', file = sys.stderr)\n",
    "            return None\n",
    "        elif e.e.code == 404:\n",
    "            print ('Encountered 404 Error (Not Found)', file = sys.stderr)\n",
    "            return None\n",
    "        elif e.e.code == 429: \n",
    "            print ('Encountered 429 Error (Rate Limit Exceeded)', file = sys.stderr)\n",
    "            if sleep_when_rate_limited:\n",
    "                print (\"Retrying in 15 minutes...ZzZ...\", file = sys.stderr)\n",
    "                sys.stderr.flush()\n",
    "                time.sleep(60*15 + 5)\n",
    "                print ('...ZzZ...Awake now and trying again.', file = sys.stderr)\n",
    "                return 2\n",
    "            else:\n",
    "                raise e # Caller must handle the rate limiting issue\n",
    "        elif e.e.code in (500, 502, 503, 504):\n",
    "            print ('Encountered %i Error. Retrying in %i seconds' % \\\n",
    "                (e.e.code, wait_period), file = sys.stderr)\n",
    "            time.sleep(wait_period)\n",
    "            wait_period *= 1.5\n",
    "            return wait_period\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "    # End of nested helper function\n",
    "    \n",
    "    wait_period = 2 \n",
    "    error_count = 0 \n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            return twitter_api_func(*args, **kw)\n",
    "        except twitter.api.TwitterHTTPError as e:\n",
    "            error_count = 0 \n",
    "            wait_period = handle_twitter_http_error(e, wait_period)\n",
    "            if wait_period is None:\n",
    "                return\n",
    "        except URLError as e:\n",
    "            error_count += 1\n",
    "            time.sleep(wait_period)\n",
    "            wait_period *= 1.5\n",
    "            print (\"URLError encountered. Continuing.\", file = sys.stderr)\n",
    "            if error_count > max_errors:\n",
    "                print (\"Too many consecutive errors...bailing out.\", file = sys.stderr)\n",
    "                raise\n",
    "        except BadStatusLine as e:\n",
    "            error_count += 1\n",
    "            time.sleep(wait_period)\n",
    "            wait_period *= 1.5\n",
    "            print (\"BadStatusLine encountered. Continuing.\", file = sys.stderr)\n",
    "            if error_count > max_errors:\n",
    "                print (\"Too many consecutive errors...bailing out.\", file = sys.stderr)\n",
    "                raise\n",
    "                \n",
    "#--------------------------------------------------------------------------------------\n",
    "# The purpose of the get_friends_followers_ids() function is to use the screen_name of\n",
    "# the twitter user passed in to find all friends and followers, find the intersection of\n",
    "# the two lists and then to print 20 friends, print 20 followers and print the mutual list.\n",
    "#\n",
    "\n",
    "def get_friends_followers_ids(twitter_api, screen_name=None, user_id=None,\n",
    "                              friends_limit=maxsize, followers_limit=maxsize):\n",
    "    \n",
    "    # Must have either screen_name or user_id (logical xor)\n",
    "    assert (screen_name != None) != (user_id != None), \\\n",
    "    \"Must have screen_name or user_id, but not both\"\n",
    "    \n",
    "    # See https://dev.twitter.com/docs/api/1.1/get/friends/ids and\n",
    "    # https://dev.twitter.com/docs/api/1.1/get/followers/ids for details\n",
    "    # on API parameters\n",
    "    \n",
    "    # Use partial pattern to make it more convenient to set up initial parameters here for function calls later\n",
    "    get_friends_ids = partial(make_twitter_request, twitter_api.friends.ids, \n",
    "                              count=5000)\n",
    "                              \n",
    "    get_followers_ids = partial(make_twitter_request, twitter_api.followers.ids, \n",
    "                               count=5000)\n",
    "\n",
    "    friends_ids, followers_ids = [], []\n",
    "    \n",
    "    for twitter_api_func, limit, ids, label in [\n",
    "                    [get_friends_ids, friends_limit, friends_ids, \"friends\"], \n",
    "                    [get_followers_ids, followers_limit, followers_ids, \"followers\"]\n",
    "                ]:\n",
    "        \n",
    "        if limit == 0: continue\n",
    "        \n",
    "        cursor = -1\n",
    "        while cursor != 0:\n",
    "        \n",
    "            # Use make_twitter_request via the partially bound callable...\n",
    "            if screen_name: \n",
    "                response = twitter_api_func(screen_name=screen_name, cursor=cursor)\n",
    "            else: # user_id\n",
    "                response = twitter_api_func(user_id=user_id, cursor=cursor)\n",
    "\n",
    "            if response is not None:\n",
    "                ids += response['ids']\n",
    "                cursor = response['next_cursor']\n",
    "        \n",
    "            print ('Fetched {0} total {1} ids for {2}'.format(len(ids), \n",
    "                                                    label, (user_id or screen_name)), file = sys.stderr)\n",
    "        \n",
    "            # XXX: You may want to store data during each iteration to provide an \n",
    "            # an additional layer of protection from exceptional circumstances\n",
    "        \n",
    "            if len(ids) >= limit or response is None:\n",
    "                break\n",
    "\n",
    "    # Do something useful with the IDs, like store them to disk...\n",
    "    \n",
    "    # Dump freinds and followers data to json file in json format    \n",
    "    with open('friends_ids.txt', 'w') as friends_outfile, open('followers_ids.txt', 'w') as followers_outfile:\n",
    "        json.dump(friends_ids, friends_outfile)  \n",
    "        json.dump(followers_ids, followers_outfile)\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "    # Note: by opening file using 'with', no need to close() the file. It happens implicitly.\n",
    "    \n",
    "    # Use python to get the intersection of the friends_ids and followers_ids\n",
    "    intersect_ids = list(set(friends_ids) & set(followers_ids))\n",
    "\n",
    "    print (\"*** HERE are ids of mutual friends followers of \", famous_person, \" *** \")\n",
    "    print (intersect_ids)\n",
    "\n",
    "    # Dump intersection data to json file in json format        \n",
    "    with open('intersect_ids.txt', 'w') as intersect_ff_outfile:\n",
    "        json.dump(intersect_ids, intersect_ff_outfile)\n",
    "    \n",
    "    # Note: by opening file using 'with', no need to close() the file. It happens implicitly.\n",
    "\n",
    "    return friends_ids[:friends_limit], followers_ids[:followers_limit]\n",
    "\n",
    "\n",
    "twitter_api = oauth_login()\n",
    "\n",
    "famous_person = \"@JohnHanke\"\n",
    "num_friends = 22\n",
    "num_followers  = 22\n",
    "\n",
    "friends_ids, followers_ids = get_friends_followers_ids(twitter_api, \n",
    "                                                       screen_name=famous_person, \n",
    "                                                       friends_limit=num_friends, \n",
    "                                                       followers_limit=num_followers)\n",
    "\n",
    "\n",
    "\n",
    "#print (\"*** HERE are \", num_friends, \" ids of friends of \", famous_person, \" *** \")\n",
    "#print (friends_ids)\n",
    "#print (\"*** HERE are \", num_followers, \" ids of followers of \", famous_person, \" *** \")\n",
    "#print (followers_ids)\n",
    "\n",
    "#print ()\n",
    "#print (\"*** Let's see them in Pretty Tables ! Doesn't this look Better ? *** \")\n",
    "#print ()\n",
    "\n",
    "#--------------------------------------------------------------------------------------\n",
    "# Prepare and print Pretty Table for FRIENDS\n",
    "\n",
    "friend_user_objects = twitter_api.users.lookup(user_id = json.dumps(friends_ids))\n",
    "\n",
    "fr_ids = [ fr['id'] \n",
    "           for fr in friend_user_objects ]\n",
    "    \n",
    "fr_names = [ fr['screen_name'] \n",
    "             for fr in friend_user_objects ]\n",
    "    \n",
    "pt = PrettyTable()\n",
    "pt.add_column(\"Friend ID\", fr_ids)\n",
    "pt.add_column(\"Friend Screen Name\", fr_names)\n",
    "    \n",
    "pt.align = 'r'\n",
    "print(pt)\n",
    "\n",
    "#--------------------------------------------------------------------------------------\n",
    "# Prepare and print Pretty Table for FOLLOWERS\n",
    "\n",
    "follower_user_objects = twitter_api.users.lookup(user_id = json.dumps(followers_ids))\n",
    "\n",
    "f_ids = [ fr['id'] \n",
    "           for fr in follower_user_objects ]\n",
    "    \n",
    "f_names = [ fr['screen_name'] \n",
    "             for fr in follower_user_objects ]\n",
    "    \n",
    "pt = PrettyTable()\n",
    "pt.add_column(\"Follower ID\", f_ids)\n",
    "pt.add_column(\"Follower Screen Name\", f_names)\n",
    "    \n",
    "pt.align = 'r'\n",
    "print(pt)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compute the mutual friends within the two groups, i.e., the users who are in both friend list and follower list, plot their ID numbers and screen names in a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n",
      "|                 ID |   Screen Name |\n",
      "+--------------------+---------------+\n",
      "| 769080580302569473 |   AgentKodama |\n",
      "|          130379779 | MichiganCorps |\n",
      "+--------------------+---------------+\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "# The intersection of the friends and followers ids collected in the above example were saved to a file.  \n",
    "# Note : This operation was performed as a poll or a one time request which has a limit on count of 5000 by design.\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "#print (\"*** Intersection of friends and followers : \")\n",
    "with open('intersect_ids.txt','r') as user_ids:\n",
    "    common_user_ids = json.load(user_ids)\n",
    "    common_users = twitter_api.users.lookup(user_id = common_user_ids)\n",
    " \n",
    "\n",
    "# Dump data to json file in json format    \n",
    "with open('intersect_users.txt', 'w') as outfile:\n",
    "    json.dump(common_users, outfile)# Therefore, the intersection of the resulting sets is not actually the complete list of mutual friends/followers of @JohnHanke,\n",
    "# rather it is an intersection of the two sets collected above.\n",
    "\n",
    "  \n",
    "\n",
    "# Print results\n",
    "with open('intersect_users.txt', 'r') as json_user_data:\n",
    "    json_users = json.load(json_user_data)\n",
    "    \n",
    "    # Print one to test\n",
    "    # print(json.dumps(json_users[0]['screen_name'], indent=4))\n",
    "    \n",
    "    # Prepare and print Pretty Table\n",
    "    mutual_ids = [ mutual_person['id'] \n",
    "                   for mutual_person in json_users ]\n",
    "    \n",
    "    mutual_names = [ mutual_person['screen_name'] \n",
    "                   for mutual_person in json_users ]\n",
    "    \n",
    "    pt = PrettyTable()\n",
    "    pt.add_column(\"ID\", mutual_ids)\n",
    "    pt.add_column(\"Screen Name\", mutual_names)\n",
    "    \n",
    "    pt.align = 'r'\n",
    "    print(pt)\n",
    "    \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*------------------------\n",
    "\n",
    "# Problem 4: Business question \n",
    "\n",
    "Run some additional experiments with your data to gain familiarity with the twitter data and twitter API.\n",
    "\n",
    "* Come up with a business question that Twitter data could help answer.\n",
    "* Decribe the business case.\n",
    "* How could Twitter data help a company decide how to spend its resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------\n",
    "# Your code starts here\n",
    "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
    "#   Please feel free to add more cells below this cell if necessary\n",
    "\n",
    "mongo_db = 'stream_search_results'\n",
    "mongo_db_coll = 'StreamingTweet'\n",
    "\n",
    "# Connects to the MongoDB server running on \n",
    "# localhost:27017 by default\n",
    "    \n",
    "client = pymongo.MongoClient()\n",
    "    \n",
    "# Get a reference to a particular database\n",
    "    \n",
    "db = client[mongo_db]\n",
    "    \n",
    "# Reference a particular collection in the database\n",
    "coll = db[mongo_db_coll]\n",
    "\n",
    "cursor = coll.find({})\n",
    "\n",
    "# The following code utilizes GeoJSON for examining coordinates of a tweet (where they exist)\n",
    "# The data file that is output can be used to view tweets on a map by geojson.io\n",
    "\n",
    "geo_data = {\n",
    "    \"type\": \"FeatureCollection\",\n",
    "    \"features\": []\n",
    "    }\n",
    "for tweet in cursor:\n",
    "    if tweet.get('coordinates'):\n",
    "        geo_json_feature = {\n",
    "                            \"type\": \"Feature\",\n",
    "                            \"geometry\": tweet['coordinates'],\n",
    "                            \"properties\": {\n",
    "                                           \"text\": tweet['text'],\n",
    "                                           \"created_at\": tweet['created_at']\n",
    "                                          }\n",
    "                            }\n",
    "        geo_data['features'].append(geo_json_feature)\n",
    " \n",
    "# Save geo data\n",
    "with open('geo_data.json', 'w') as outfile:\n",
    "    json.dump(geo_data, outfile, indent=4)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-----------------\n",
    "# Done\n",
    "\n",
    "All set! \n",
    "\n",
    "** What do you need to submit?**\n",
    "\n",
    "* **Notebook File**: Save this IPython notebook, and find the notebook file in your folder (for example, \"filename.ipynb\"). This is the file you need to submit. Please make sure all the plotted tables and figures are in the notebook. If you used \"ipython notebook --pylab=inline\" to open the notebook, all the figures and tables should have shown up in the notebook.\n",
    "\n",
    "\n",
    "* **PPT Slides**: please prepare PPT slides (for 10 minutes' talk) to present about the case study . We will ask two teams which are randomly selected to present their case studies in class for this case study. \n",
    "\n",
    "* ** Report**: please prepare a report (less than 10 pages) to report what you found in the data.\n",
    "    * What data you collected? \n",
    "    * Why this topic is interesting or important to you? (Motivations)\n",
    "    * How did you analyse the data?\n",
    "    * What did you find in the data? \n",
    " \n",
    "     (please include figures or tables in the report, but no source code)\n",
    "\n",
    "Please compress all the files in a zipped file.\n",
    "\n",
    "\n",
    "** How to submit: **\n",
    "\n",
    "        Please submit through email to Prof. Paffenroth (rcpaffenroth@wpi.edu) *and* the TA Wen Liu (wliu3@wpi.edu).\n",
    "        \n",
    "** Note: Each team just needs to submits one submission **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grading Criteria:\n",
    "\n",
    "** Totoal Points: 120 **\n",
    "\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "** Notebook:  **\n",
    "    Points: 80\n",
    "\n",
    "\n",
    "    -----------------------------------\n",
    "    Qestion 1:\n",
    "    Points: 20\n",
    "    -----------------------------------\n",
    "    \n",
    "    (1) Select a topic that you are interested in.\n",
    "    Points: 6 \n",
    "    \n",
    "    (2) Use Twitter Streaming API to sample a collection of tweets about this topic in real time. (It would be recommended that the number of tweets should be larger than 200, but smaller than 1 million. Please check whether the total number of tweets collected is larger than 200?\n",
    "    Points: 10 \n",
    "    \n",
    "    \n",
    "    (3) Store the tweets you downloaded into a local file (txt file or json file)\n",
    "    Points: 4 \n",
    "    \n",
    "    \n",
    "    -----------------------------------\n",
    "    Qestion 2:\n",
    "    Points: 20\n",
    "    -----------------------------------\n",
    "    \n",
    "    1. Word Count\n",
    "\n",
    "    (1) Use the tweets you collected in Problem 1, and compute the frequencies of the words being used in these tweets.\n",
    "    Points: 4 \n",
    "\n",
    "    (2) Plot a table of the top 30 words with their counts \n",
    "    Points: 4 \n",
    "    \n",
    "    2. Find the most popular tweets in your collection of tweets\n",
    "    plot a table of the top 10 tweets that are the most popular among your collection, i.e., the tweets with the largest number of retweet counts.\n",
    "    Points: 4 \n",
    "    \n",
    "    3. Find the most popular Tweet Entities in your collection of tweets\n",
    "\n",
    "    (1) plot a table of the top 10 hashtags, \n",
    "    Points: 4 \n",
    "\n",
    "    (2) top 10 user mentions that are the most popular in your collection of tweets.\n",
    "    Points: 4 \n",
    "    \n",
    "    \n",
    "    -----------------------------------\n",
    "    Qestion 3:\n",
    "    Points: 20\n",
    "    -----------------------------------\n",
    "    \n",
    "    (1) choose a popular twitter user who has many followers, such as \"ladygaga\".\n",
    "    Points: 4 \n",
    "\n",
    "    (2) Get the list of all friends and all followers of the twitter user.\n",
    "    Points: 4 \n",
    "\n",
    "    (3) Plot 20 out of the followers, plot their ID numbers and screen names in a table.\n",
    "    Points: 4 \n",
    "\n",
    "    (4) Plot 20 out of the friends (if the user has more than 20 friends), plot their ID numbers and screen names in a table.\n",
    "    Points: 4 \n",
    "    \n",
    "    (5) Compute the mutual friends within the two groups, i.e., the users who are in both friend list and follower list, plot their ID numbers and screen names in a table\n",
    "    Points: 4 \n",
    "  \n",
    "    -----------------------------------\n",
    "    Qestion 4:  Business question\n",
    "    Points: 20\n",
    "    -----------------------------------\n",
    "        Novelty: 10\n",
    "        Interestingness: 10\n",
    "    -----------------------------------\n",
    "    Run some additional experiments with your data to gain familiarity with the twitter data ant twitter API.  Come up with a business question and describe how Twitter data can help you answer that question.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "** Report: communicate the results**\n",
    "    Points: 20\n",
    "\n",
    "(1) What data you collected?\n",
    "    Points: 5 \n",
    "\n",
    "(2) Why this topic is interesting or important to you? (Motivations)\n",
    "    Points: 5 \n",
    "\n",
    "(3) How did you analyse the data?\n",
    "    Points: 5 \n",
    "\n",
    "(4) What did you find in the data?\n",
    "(please include figures or tables in the report, but no source code)\n",
    "    Points: 5 \n",
    "\n",
    "\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "** Slides (for 10 minutes of presentation): Story-telling **\n",
    "    Points: 20\n",
    "\n",
    "\n",
    "1. Motivation about the data collection, why the topic is interesting to you.\n",
    "    Points: 5 \n",
    "\n",
    "2. Communicating Results (figure/table)\n",
    "    Points: 10 \n",
    "\n",
    "3. Story telling (How all the parts (data, analysis, result) fit together as a story?)\n",
    "    Points: 5 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
